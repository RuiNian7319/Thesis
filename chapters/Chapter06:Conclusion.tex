- RL is excellent in an highly mathematical complex problem that you have a simulator for like games.  But such processes don't exist in real life.
- MPC is the ideal outcome of RL everytime.
- RL has more exotic applications than control, and its value is most likely not in control.

\section{Concluding Remarks}











As a final remark, the truly unique characteristic of RL that makes it the closest thing to real artificial intelligence is its \textit{general nature}, allowing for learning of many things through a general algorithm. Although modern RL still faces many challenges, the most interesting fact about artificial general intelligence is that it is \textit{eventually} scientifically achievable. Unlike galactic teleporters or other wild fantasies from science fiction literature, artificial general intelligence is proven to exist, currently within us!  The last step is \textit{merely} to reverse engineer human psychology. And when such a task if finally conquered, the concepts reinforcement learning will, \textit{without a doubt}, reside as its central algorithm.

\section{Future Extensions}
\subsection{RL-MPC - An unified approach}
In terms of control, one possible future project would be to combine RL and MPC into one unifying algorithm.  Currently, RL is a newer field of research and lacks industrial support. Therefore, most plant managers are skeptical of its performance in direct closed-loop control. On the other hand, linear MPCs have many applications in process control but the applications of its non-linear counterpart is still relatively scarce. From an engineering prospective, non-linear MPC is vastly superior because linear processes do not exist in the real world. One factor barring non-linear MPCs from implementation is its much higher computational burden. Mathematically, both linear and non-linear programming are solved in an iterative approach and the convergence is dependent on the initial guess.  By leveraging RL to provide initial guesses to the non-linear MPC, the computational time should be substantially faster, leading to the viability of non-linear MPCs. Theoretically, the initial guess provided by a perfectly trained RL should be exactly the optimal solution, greatly reducing the iterative procedure required by non-linear programming.  

\subsection{Meta-learning in reinforcement learning}
Initially, RL agents must be trained on the desired task to obtain optimal performance.  Due to the low data efficiency of modern RL algorithms, thousands of interactions may be required before the agent learns something meaningful.  Such a requirement is infeasible in real life applications; thus, a representative simulator of the environment is first identified to pre-train the agent in simulation.  Then, the agent is implemented to the real process for control and optimization purposes.  Without a doubt, there will be off-sets between the identified model and the real process. Hence, RL will also need an initial calibration period to directly adapt its learned policy from the simulator onto the real process.  The length of this adaptation period is dictated by the accuracy of the simulator.  Unfortunately, there are many processes that are nearly impossible to identify accurately.  In such scenarios, the calibration period itself may be in-feasibly long. Meta reinforcement learning is a new field that aims to significantly reduce this calibration time.

In meta reinforcement learning, many \textit{different} simulators of the environments are built to capture model uncertainty.  For example in a refinery, one model is built for winter ambient temperatures while another is built for the summer.  There could also be different models built for different compositions of crude oil.  The goal of the agent is to adapt to new similar environments quickly, even when the exact same task was not trained for.  One future project could be to explore training an agent on many models with modeling errors, and then ultimately implementing the agent onto the real system to identify its adaptation speed.  If successful, such a application holds many implications for RL in process control because one general agent could be used to control many different similar systems.